<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gpt on</title><link>https://coolbeevip.github.io/tags/gpt/</link><description>Recent content in Gpt on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 2024 20:24:14 +0800</lastBuildDate><atom:link href="https://coolbeevip.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml"/><item><title>Prompt Engineering Types</title><link>https://coolbeevip.github.io/posts/gpt/prompt-engineering/</link><pubDate>Mon, 01 Jan 2024 20:24:14 +0800</pubDate><guid>https://coolbeevip.github.io/posts/gpt/prompt-engineering/</guid><description>Types Zero-Shot Learning One-Shot Learning Few-Shot Learning Chain-of-Thought Prompting Iterative Prompting Negative Prompting Hybrid Prompting Prompt Chaining Consistency Prompting Knowledge Prompting Tree of Thoughts(ToT) ReAct Prompting Directional Stimulus Prompting Zero-Show Learning 在没有任何示例的情况下给人工智能一个任务。假设人工智能事先不知道任务，你可以详细描述你想要什么。
Prompt: “请解释什么是大语言模型”
GPT 3.5 Response:
大语言模型是一种基于深度学习技术的人工智能模型，旨在生成自然语言文本。这类模型通常具有非常大的神经网络结构和参数数量，可以对大规模的语料库进行训练，并尝试预测下一个单词或句子的可能性。大语言模型已经被广泛应用于自然语言处理、机器翻译、自动文摘、语音识别等领域，在提高计算机语言理解和生成能力方面起到了重要的作用。常见的大语言模型包括BERT、GPT-2、ELMo等
One-Shot Learning 您随提示提供了一个示例。这有助于人工智能理解您期望的上下文或格式。
Prompt: “人工智能中的基础模型是指像 GPT-3 这样的模型、 这样的模型，它在大型数据集上经过训练，可适用于各种任务。 在这种情况下，请解释什么是 BERT”
GPT 3.5 Response:
BERT（Bidirectional Encoder Representations from Transformers）是一种基于变形器（Transformer）结构的自然语言处理（NLP）模型，它是由Google开发的，可以进行预训练和微调。BERT的主要功能是用来处理自然语言理解任务，例如问答、语义理解、文本分类等。不同于传统的自然语言处理模型，BERT采用了双向的上下文理解技术，可以更好地理解和适应语言上下文。BERT在大型数据集上进行预训练，可以得到更为优秀的模型表现，在各种自然语言处理任务上具有很高的精度和普适性。
Few-Shot Learning 提供一些示例（通常是 2-5 个）来帮助 AI 理解您正在寻找的响应的模式或风格。
Prompt: “GPT-3 等基础模型用于自然语言处理，而 DALL-E 等模型用于图像生成。基础模型如何应用于机器人领域?”
GPT 3.5 Response:</description></item></channel></rss>