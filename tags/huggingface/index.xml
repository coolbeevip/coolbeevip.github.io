<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Huggingface on</title><link>https://coolbeevip.github.io/tags/huggingface/</link><description>Recent content in Huggingface on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 05 Jan 2025 13:24:14 +0800</lastBuildDate><atom:link href="https://coolbeevip.github.io/tags/huggingface/index.xml" rel="self" type="application/rss+xml"/><item><title>Benchmark for BAAI/bge-3m on an Nvidia A800/CPU/Mac M1</title><link>https://coolbeevip.github.io/posts/huggingface/benchmark-embeddings/</link><pubDate>Sun, 05 Jan 2025 13:24:14 +0800</pubDate><guid>https://coolbeevip.github.io/posts/huggingface/benchmark-embeddings/</guid><description>在同一个服务器上测试 CPU / GPU 性能差异
测试代码 import time import sentence_transformers import torch if __name__ == &amp;#34;__main__&amp;#34;: device = &amp;#34;cuda&amp;#34; if torch.cuda.is_available() else &amp;#34;cpu&amp;#34; embedding = sentence_transformers.SentenceTransformer( model_name_or_path=&amp;#34;/Volumes/SD/huggingface-models/bge-m3&amp;#34;, cache_folder=&amp;#34;/Volumes/SD/huggingface-models&amp;#34;, device=device ) total = 10000 batch_size = 100 start_time = time.time() sentences = [&amp;#34;I am AnCopilot, nice to meet you!&amp;#34;] for i in range(total // batch_size): embedding.encode(sentences * batch_size, normalize_embeddings=True) print(f&amp;#34;{i + 1} / {total // batch_size}&amp;#34;) end_time = time.time() total_time = end_time - start_time average_time = total_time / total throughput = total / total_time print(f&amp;#34;Device {device}&amp;#34;) print(f&amp;#34;Total {total} sentences&amp;#34;) print(f&amp;#34;Batch size: {batch_size}&amp;#34;) print(f&amp;#34;Total time: {total_time:.</description></item></channel></rss>